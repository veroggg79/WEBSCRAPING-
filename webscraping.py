# -*- coding: utf-8 -*-
"""ultimos algoritmos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3JZ2-UAUDMwuzBn2obdW6mKpxpg8y5-

#### Análisis de post en twitter de 5 perfiles de twitter

___________________________________

### 1. Consolidar las bases de datos descargadas en un dataframe

A continuación se consolidan las bases de datos descargadas.

Pastel:
https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe
"""

import pandas as pd
import glob

path = r'C:\Users\ESTUDIANTE\Scrap Twitter\twint\data\HuaweiMobile'
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

huawei = pd.concat(li, axis=0, ignore_index=True)

import pandas as pd
import glob

path = r'C:\Users\ESTUDIANTE\Scrap Twitter\twint\data\Motorola'
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

motorola = pd.concat(li, axis=0, ignore_index=True)

import pandas as pd
import glob

path = r'C:\Users\ESTUDIANTE\Scrap Twitter\twint\data\samsung'
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

samsung = pd.concat(li, axis=0, ignore_index=True)

import pandas as pd
import glob

path = r'C:\Users\ESTUDIANTE\Scrap Twitter\twint\data\XiaomiColombia'
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

xiaomi = pd.concat(li, axis=0, ignore_index=True)

"""### 2. Importar los paquetes
A continuación debemos importar los paquetes instalados a nuestro notebook a través de la ejecución del siguiente comando:

"""

#Terminal: pip install -U pip setuptools wheel
#Terminal: pip install -U spacy
#Terminal: python -m spacy download en_core_web_sm
#Terminal: python -m spacy download en_core_web_sm
#Terminal pip install spacy_spanish_lemmatizer
#Terminal python -m spacy_spanish_lemmatizer download wiki
#Terminal: pip install wordcloud

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import nltk
import unidecode
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
from unicodedata import normalize
import csv
from dateutil import parser
import matplotlib.pyplot as plt
from nltk.probability import FreqDist
import spacy
nlp = spacy.load("en_core_web_sm")
import en_core_web_sm
nlp = en_core_web_sm.load()
import spacy
import spacy_spanish_lemmatizer
import spacy
import spacy_spanish_lemmatizer
import nltk
nltk.download('punkt')
import nltk
nltk.download('stopwords')
from wordcloud import WordCloud
import nltk
nltk.download('averaged_perceptron_tagger')
import nltk
from nltk import word_tokenize
import re
import pickle
#from pysentimiento import SentimentAnalyzer
#analyzer = SentimentAnalyzer(lang="es")
import pandas as pd
import re
from unicodedata import normalize
import emoji
from wordcloud import WordCloud

"""### 3. Almacenar los data frames como listas
Los archivos que llevamos a los dataframes, vamos a almacenarlos como listas, para posteriormente analizarlas.

Pastel: https://stackoverflow.com/questions/40918503/how-to-read-a-utf-8-encoded-text-file-using-python
"""

type(huawei)

type(motorola)

type(samsung)

type(xiaomi)

display(huawei.head(2))

display(motorola.head(2))

display(samsung.head(2))

display(xiaomi.head(2))

huawei_df = huawei.tweet.drop_duplicates()
huawei_df = pd.DataFrame(huawei_df)
display(huawei_df.tail(2))

motorola_df = motorola.tweet.drop_duplicates()
motorola_df = pd.DataFrame(motorola_df)
display(motorola_df.tail(2))

samsung_df = samsung.tweet.drop_duplicates()
samsung_df = pd.DataFrame(samsung_df)
display(samsung_df.tail(2))

xiaomi_df = xiaomi.tweet.drop_duplicates()
xiaomi_df = pd.DataFrame(xiaomi_df)
display(xiaomi_df.tail(2))

huawei_df["tweet"].str.len()

motorola_df["tweet"].str.len()

samsung_df["tweet"].str.len()

xiaomi_df["tweet"].str.len()

huawei = list(huawei_df["tweet"].unique())
print(huawei[0:2])

motorola = list(motorola_df["tweet"].unique())
print(motorola[0:2])

samsung = list(samsung_df["tweet"].unique())
print(samsung[0:2])

xiaomi = list(xiaomi_df["tweet"].unique())
print(xiaomi[0:2])

len(huawei)

len(motorola)

len(samsung)

len(xiaomi)

type(huawei)

type(motorola)

type(samsung)

type(xiaomi)

"""### 4. Tamaño de los post
A continuación vamos a revisar la longitud de los post de cada una de las Universidades, este comparativo lo vamos a hacer a traves de diagramas de violin, para identificar valores extremos, distribucion de las longitudes, media y rango intercuartilico.
"""

huawei_df.columns  = ["post"]
huawei_len = pd.DataFrame(columns = ["empresa", "len"])
huawei_len["len"] = huawei_df["post"].str.len()
huawei_len["empresa"] = "huawei"

motorola_df.columns  = ["post"]
motorola_len = pd.DataFrame(columns = ["empresa", "len"])
motorola_len["len"] = motorola_df["post"].str.len()
motorola_len["empresa"] = "motorola"

samsung_df.columns  = ["post"]
samsung_len = pd.DataFrame(columns = ["empresa", "len"])
samsung_len["len"] = samsung_df["post"].str.len()
samsung_len["empresa"] = "samsung"

xiaomi_df.columns  = ["post"]
xiaomi_len = pd.DataFrame(columns = ["empresa", "len"])
xiaomi_len["len"] = xiaomi_df["post"].str.len()
xiaomi_len["empresa"] = "xiaomi"

frames = [huawei_len, motorola_len, samsung_len, xiaomi_len]
u_len = pd.concat(frames)

#Grafico de violin en seaborn
import seaborn as sns
sns.set(rc={'figure.figsize':(13.7,8.27)})
ax = sns.violinplot(x="empresa", y="len", data=u_len, palette="muted")

"""### 5. Unigramas
A continuación vamos a identificar las palabras que mayor ocurrencia presentan de manera individual sin haber hecho depuracion de los post, es decir los unigramas mas frecuentes en las bases de datos.
"""

stop_words=set(stopwords.words("spanish"))
print(stop_words)

from nltk import ngrams

dfs = [huawei_df]

for df, dataset in zip(dfs, ['huawei_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['huawei']):
        words = ' '.join(df_new['post']).lower().split()
        n = 1
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los unigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [motorola_df]

for df, dataset in zip(dfs, ['motorola_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['motorola']):
        words = ' '.join(df_new['post']).lower().split()
        n = 1
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los unigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [samsung_df]

for df, dataset in zip(dfs, ['samsung_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['samsung']):
        words = ' '.join(df_new['post']).lower().split()
        n = 1
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los unigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [xiaomi_df]

for df, dataset in zip(dfs, ['xiaomi_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['xiaomi']):
        words = ' '.join(df_new['post']).lower().split()
        n = 1
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los unigramas más comunes en los post de la "+tag+ " son: ", common)

"""### 6. Bigramas
A continuación vamos a identificar las palabras que mayor ocurrencia presentan por pares, es decir los bigramas mas frecuentes en las bases de datos.
"""

from nltk import ngrams

dfs = [huawei_df]

for df, dataset in zip(dfs, ['huawei_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['huawei']):
        words = ' '.join(df_new['post']).lower().split()
        n = 2
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los bigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [motorola_df]

for df, dataset in zip(dfs, ['motorola_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['motorola']):
        words = ' '.join(df_new['post']).lower().split()
        n = 2
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los bigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [samsung_df]

for df, dataset in zip(dfs, ['samsung_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['samsung']):
        words = ' '.join(df_new['post']).lower().split()
        n = 2
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los bigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [xiaomi_df]

for df, dataset in zip(dfs, ['xiaomi_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['xiaomi']):
        words = ' '.join(df_new['post']).lower().split()
        n = 2
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los bigramas más comunes en los post de la "+tag+ " son: ", common)

"""### 7. Trigramas
A continuación vamos a identificar las palabras que mayor ocurrencia presentan por trios o ternas, es decir los triigramas mas frecuentes en las bases de datos.
"""

from nltk import ngrams

dfs = [huawei_df]

for df, dataset in zip(dfs, ['huawei_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['huawei']):
        words = ' '.join(df_new['post']).lower().split()
        n = 3
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los trigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [motorola_df]

for df, dataset in zip(dfs, ['motorola_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['motorola']):
        words = ' '.join(df_new['post']).lower().split()
        n = 3
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los trigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [samsung_df]

for df, dataset in zip(dfs, ['samsung_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['samsung']):
        words = ' '.join(df_new['post']).lower().split()
        n = 3
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los trigramas más comunes en los post de la "+tag+ " son: ", common)

from nltk import ngrams

dfs = [xiaomi_df]

for df, dataset in zip(dfs, ['xiaomi_df']):
    new_dfs = [df]
    print('Post de', dataset)
    for df_new, tag in zip(new_dfs, ['xiaomi']):
        words = ' '.join(df_new['post']).lower().split()
        n = 3
        non_stopwords = [word for word in words if word not in stop_words]
        bigrams = ngrams(non_stopwords, n)
        common = pd.Series(bigrams).value_counts()[:20].index.tolist()
        print("Los trigramas más comunes en los post de la "+tag+ " son: ", common)

"""### 8. Estandarizar mayúsculas y minúsculas
Iniciaremos este prosamsungo llevando todas las cadenas de texto a minúsculas.
"""

huawei_lower = str(huawei).lower()
print(huawei_lower[:600])

motorola_lower = str(motorola).lower()
print(motorola_lower[:600])

samsung_lower = str(samsung).lower()
print(samsung_lower[:600])

xiaomi_lower = str(xiaomi).lower()
print(xiaomi_lower[:600])

"""### 9. Estandarizar acentos
A continuación eliminaremos los acentos o tildes de las palabras que componen nuestras cadenas de texto, sin eliminar las ñ.

El código es largo, entonsamsung usaremos el caracter (\\ 'backslash') para dividir el código en varias lineas.

Pastel: https://es.stackoverflow.com/questions/135707/c%C3%B3mo-puedo-reemplazar-las-letras-con-tildes-por-las-mismas-sin-tilde-pero-no-l
"""

huawei_lower_unaccented = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", \
                               normalize( "NFD", huawei_lower), 0, re.I)
huawei_lower_unaccented = normalize( 'NFC', huawei_lower_unaccented)
print(huawei_lower_unaccented[0:400])

motorola_lower_unaccented = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", \
                               normalize( "NFD", motorola_lower), 0, re.I)
motorola_lower_unaccented = normalize( 'NFC', motorola_lower_unaccented)
print(motorola_lower_unaccented[0:400])

samsung_lower_unaccented = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", \
                               normalize( "NFD", samsung_lower), 0, re.I)
samsung_lower_unaccented = normalize( 'NFC', samsung_lower_unaccented)
print(samsung_lower_unaccented[0:400])

xiaomi_lower_unaccented = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", \
                               normalize( "NFD", xiaomi_lower), 0, re.I)
xiaomi_lower_unaccented = normalize( 'NFC', xiaomi_lower_unaccented)
print(xiaomi_lower_unaccented[0:400])

"""### 10. Eliminar URLs de las cadenas de texto
Para continuar con la limpieza vamos a eliminar las URLs que se encuentran en las cadenas de texto.

Pastel: https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python
"""

huawei_lower_unaccented_unurls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', huawei_lower_unaccented, \
                                      flags=re.MULTILINE)
print(huawei_lower_unaccented_unurls[:600])

motorola_lower_unaccented_unurls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', motorola_lower_unaccented, \
                                      flags=re.MULTILINE)
print(motorola_lower_unaccented_unurls[:600])

samsung_lower_unaccented_unurls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', samsung_lower_unaccented, \
                                      flags=re.MULTILINE)
print(samsung_lower_unaccented_unurls[:600])

xiaomi_lower_unaccented_unurls = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', xiaomi_lower_unaccented, \
                                      flags=re.MULTILINE)
print(xiaomi_lower_unaccented_unurls[:600])

"""### 11. Extraer emojis, diferente a emoticones
Los emoticones son signos de puntuación, letras y números que se utilizan para crear íconos pictóricos que generalmente denotan una emoción o un sentimiento. Mientras que los emojis son un invento un poco más reciente. Su nombre se compone de las palabras, en japonés e, que significa imagen, y moji, personaje y son pictografías de caras, objetos y símbolos.

A continuación vamos a tratar de extraerlos de las cadenas de texto disponibles:https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text
"""

emojis_huawei = ''.join(c for c in huawei_lower_unaccented_unurls if c in emoji.EMOJI_DATA)
print(emojis_huawei[:])

emojis_motorola = ''.join(c for c in motorola_lower_unaccented_unurls if c in emoji.EMOJI_DATA)
print(emojis_motorola[:])

emojis_samsung = ''.join(c for c in samsung_lower_unaccented_unurls if c in emoji.EMOJI_DATA)
print(emojis_samsung[:])

emojis_xiaomi = ''.join(c for c in xiaomi_lower_unaccented_unurls if c in emoji.EMOJI_DATA)
print(emojis_xiaomi[:])

"""### 12. Eliminar caracteres especiales
Luego de remover las URLs las cadenas de texto a continuación eliminaremos los caracteres especiales manteneniendo el espacio, las "ñ´s" y la estructura de la cadena de los 5 corpus de data. Tambien eliminaremos las expresiones "ver mas", "edu.co", "am", "pm" y "m" que corresponde con información vacia.

Pastel: https://www.it-swarm-es.com/es/python/como-eliminar-caracteres-especiales-excepto-el-espacio-de-un-archivo-en-python/831145893/
"""

huawei_lower_unaccented_unurls = huawei_lower_unaccented_unurls.replace("ñ", "X")
for k in huawei_lower_unaccented_unurls.split("\n"):
    huawei_lower_unaccented_unurls_unspecial = [re.sub(r"[^a-zA-Z0-9]+", ' ', k) \
                                              for k in huawei_lower_unaccented_unurls.split("\n")]

huawei_lower_unaccented_unurls_unspecial = \
str(huawei_lower_unaccented_unurls_unspecial).replace("['","").replace("', '","").replace(" ver ", ""). \
replace(" mas ", "").replace(" am ","").replace(" pm ","").replace(" m ","").replace(" edu ",""). \
replace(" co ","").replace("X", "ñ").replace(" n ", "").replace(" u200d ", "").replace("www", "")

print(huawei_lower_unaccented_unurls_unspecial[:600])

motorola_lower_unaccented_unurls = motorola_lower_unaccented_unurls.replace("ñ", "X")
for k in motorola_lower_unaccented_unurls.split("\n"):
    motorola_lower_unaccented_unurls_unspecial = [re.sub(r"[^a-zA-Z0-9]+", ' ', k) \
                                              for k in motorola_lower_unaccented_unurls.split("\n")]

motorola_lower_unaccented_unurls_unspecial = \
str(motorola_lower_unaccented_unurls_unspecial).replace("['","").replace("', '","").replace(" ver ", ""). \
replace(" mas ", "").replace(" am ","").replace(" pm ","").replace(" m ","").replace(" edu ",""). \
replace(" co ","").replace("X", "ñ").replace(" n ", "").replace(" u200d ", "").replace("www", "")

print(motorola_lower_unaccented_unurls_unspecial[:600])

samsung_lower_unaccented_unurls = samsung_lower_unaccented_unurls.replace("ñ", "X")
for k in samsung_lower_unaccented_unurls.split("\n"):
    samsung_lower_unaccented_unurls_unspecial = [re.sub(r"[^a-zA-Z0-9]+", ' ', k) \
                                              for k in samsung_lower_unaccented_unurls.split("\n")]

samsung_lower_unaccented_unurls_unspecial = \
str(samsung_lower_unaccented_unurls_unspecial).replace("['","").replace("', '","").replace(" ver ", ""). \
replace(" mas ", "").replace(" am ","").replace(" pm ","").replace(" m ","").replace(" edu ",""). \
replace(" co ","").replace("X", "ñ").replace(" n ", "").replace(" u200d ", "").replace("www", "")

print(samsung_lower_unaccented_unurls_unspecial[:600])

xiaomi_lower_unaccented_unurls = xiaomi_lower_unaccented_unurls.replace("ñ", "X")
for k in xiaomi_lower_unaccented_unurls.split("\n"):
    xiaomi_lower_unaccented_unurls_unspecial = [re.sub(r"[^a-zA-Z0-9]+", ' ', k) \
                                              for k in xiaomi_lower_unaccented_unurls.split("\n")]

xiaomi_lower_unaccented_unurls_unspecial = \
str(xiaomi_lower_unaccented_unurls_unspecial).replace("['","").replace("', '","").replace(" ver ", ""). \
replace(" mas ", "").replace(" am ","").replace(" pm ","").replace(" m ","").replace(" edu ",""). \
replace(" co ","").replace("X", "ñ").replace(" n ", "").replace(" u200d ", "").replace("www", "")

print(xiaomi_lower_unaccented_unurls_unspecial[:600])

"""### 13. Eliminar numeros
Para completar la limpieza de las cadenas de texto, a continuación eliminaremos los numeros que contiene.

Pastel:https://stackoverflow.com/questions/12851791/removing-numbers-from-string
"""

huawei_lower_unaccented_unurls_unspecial_unnum = ''.join([i for i in huawei_lower_unaccented_unurls_unspecial \
                                                        if not i.isdigit()])
print(huawei_lower_unaccented_unurls_unspecial_unnum[:600])

motorola_lower_unaccented_unurls_unspecial_unnum = ''.join([i for i in motorola_lower_unaccented_unurls_unspecial \
                                                        if not i.isdigit()])
print(motorola_lower_unaccented_unurls_unspecial_unnum[:600])

samsung_lower_unaccented_unurls_unspecial_unnum = ''.join([i for i in samsung_lower_unaccented_unurls_unspecial \
                                                        if not i.isdigit()])
print(samsung_lower_unaccented_unurls_unspecial_unnum[:600])

xiaomi_lower_unaccented_unurls_unspecial_unnum = ''.join([i for i in xiaomi_lower_unaccented_unurls_unspecial \
                                                        if not i.isdigit()])
print(xiaomi_lower_unaccented_unurls_unspecial_unnum[:600])

"""### 14. Lematizar

La lematización reduce las palabras a su palabra base, que son lemas lingüísticamente correctos. A continuación lo aplicaremos a las cadenas de texto de las universidades. La lematización en español no es 100% exacta, por que NLTK no trae un core en español, sin embargo será util cualquier lematización que logre.

![imagen.png](attachment:imagen.png)

Pastel:https://github.com/pablodms/spacy-spanish-lemmatizer

https://stackoverflow.com/questions/39366134/how-to-store-values-retrieved-from-a-loop-in-different-variables-in-python

https://www.geeksforgeeks.org/python-program-to-convert-a-list-to-string/

Pastel restricciones lemattizar: https://python.tutorialink.com/how-to-solve-spanish-lemmatization-problems-with-spacy/
"""

huawei_lower_unaccented_unurls_unspecial_unnum_lemm = []
nlp = spacy.load("en_core_web_sm")
nlp.replace_pipe("lemmatizer", "spanish_lemmatizer")
for token in nlp(huawei_lower_unaccented_unurls_unspecial_unnum):
    huawei_lower_unaccented_unurls_unspecial_unnum_lemm.append(token.lemma_)

huawei_lower_unaccented_unurls_unspecial_unnum_lemm = ' '.join(map(str, huawei_lower_unaccented_unurls_unspecial_unnum_lemm))
print(huawei_lower_unaccented_unurls_unspecial_unnum_lemm[:400])

motorola_lower_unaccented_unurls_unspecial_unnum_lemm = []
nlp = spacy.load("en_core_web_sm")
nlp.replace_pipe("lemmatizer", "spanish_lemmatizer")
for token in nlp(motorola_lower_unaccented_unurls_unspecial_unnum):
    motorola_lower_unaccented_unurls_unspecial_unnum_lemm.append(token.lemma_)

motorola_lower_unaccented_unurls_unspecial_unnum_lemm = ' '.join(map(str, motorola_lower_unaccented_unurls_unspecial_unnum_lemm))
print(motorola_lower_unaccented_unurls_unspecial_unnum_lemm[:400])

samsung_lower_unaccented_unurls_unspecial_unnum_lemm = []
nlp = spacy.load("en_core_web_sm")
nlp.replace_pipe("lemmatizer", "spanish_lemmatizer")
for token in nlp(samsung_lower_unaccented_unurls_unspecial_unnum):
    samsung_lower_unaccented_unurls_unspecial_unnum_lemm.append(token.lemma_)

samsung_lower_unaccented_unurls_unspecial_unnum_lemm = ' '.join(map(str, samsung_lower_unaccented_unurls_unspecial_unnum_lemm))
print(samsung_lower_unaccented_unurls_unspecial_unnum_lemm[:400])

xiaomi_lower_unaccented_unurls_unspecial_unnum_lemm = []
nlp = spacy.load("en_core_web_sm")
nlp.replace_pipe("lemmatizer", "spanish_lemmatizer")
for token in nlp(xiaomi_lower_unaccented_unurls_unspecial_unnum):
    xiaomi_lower_unaccented_unurls_unspecial_unnum_lemm.append(token.lemma_)

xiaomi_lower_unaccented_unurls_unspecial_unnum_lemm = ' '.join(map(str, xiaomi_lower_unaccented_unurls_unspecial_unnum_lemm))
print(xiaomi_lower_unaccented_unurls_unspecial_unnum_lemm[:400])

"""### 15. Tokenizar

La tokenización es el primer paso en el análisis de texto. El prosamsungo de dividir un párrafo de texto en fragmentos más pequeños, como palabras u oraciones, se llama Tokenización. Token es una entidad única que está construyendo bloques para una oración o un párrafo. A continuación, presentamos la tokenización por palabras para nuestro "corpus" -i.e., Un corpus lingüístico es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua- de data.

"""

huawei_tokenized_word = word_tokenize(str(huawei_lower_unaccented_unurls_unspecial_unnum))
print(huawei_tokenized_word[0:100])

motorola_tokenized_word = word_tokenize(str(motorola_lower_unaccented_unurls_unspecial_unnum))
print(motorola_tokenized_word[0:100])

samsung_tokenized_word = word_tokenize(str(samsung_lower_unaccented_unurls_unspecial_unnum))
print(samsung_tokenized_word[0:100])

xiaomi_tokenized_word = word_tokenize(str(xiaomi_lower_unaccented_unurls_unspecial_unnum))
print(xiaomi_tokenized_word[0:100])

"""### 16. Eliminar palabras irrelevantes (stop-words)
Tras tokenizar el corpus en frases y palabras, a continuación eliminaremos las palabras que son irrelevantes para el texto, conocidas como stop-words. Las palabras irrelevantes que se eliminarán son las siguientes:
"""

stop_words=set(stopwords.words("spanish"))
print(stop_words)

"""Identificadas las palabras irrelevantes, las eliminaremos de las palabras tokenizadas"""

huawei_filtered_word = []

for w in huawei_tokenized_word:
    if w not in stop_words:
        huawei_filtered_word.append(w)

print(huawei_filtered_word[:20])

motorola_filtered_word = []

for w in motorola_tokenized_word:
    if w not in stop_words:
        motorola_filtered_word.append(w)

print(motorola_filtered_word[:20])

samsung_filtered_word = []

for w in samsung_tokenized_word:
    if w not in stop_words:
        samsung_filtered_word.append(w)

print(samsung_filtered_word[:20])

xiaomi_filtered_word = []

for w in xiaomi_tokenized_word:
    if w not in stop_words:
        xiaomi_filtered_word.append(w)

print(xiaomi_filtered_word[:20])

"""### 17. Nube de palabras
Luego de tener los tokens, vamos a representar los sustantivos y adjetivos mas recurrentes a traves de una nube de palabras para identificar aquellas palabras que presentan una mayor ocurrencia, es decir las palabras más populares en las comunicaciones de cada universidad.

Pastel: https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5
"""

huawei_filtered_word_freq = FreqDist(huawei_filtered_word)
huawei_filtered_word_freq.most_common(20)

huawei_word_cloud = WordCloud(width = 3000, height = 2000, random_state=1, max_words=30, background_color='black', \
                            colormap='Set2', collocations=False).generate(str(huawei_filtered_word))
plt.imshow(huawei_word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

motorola_filtered_word_freq = FreqDist(motorola_filtered_word)
motorola_filtered_word_freq.most_common(20)

motorola_word_cloud = WordCloud(width = 3000, height = 2000, random_state=1, max_words=30, background_color='black', \
                            colormap='Set2', collocations=False).generate(str(motorola_filtered_word))
plt.imshow(motorola_word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

samsung_filtered_word_freq = FreqDist(samsung_filtered_word)
samsung_filtered_word_freq.most_common(20)

samsung_word_cloud = WordCloud(width = 3000, height = 2000, random_state=1, max_words=30, \
                           background_color='black',colormap='Set2', \
                           collocations=False).generate(str(samsung_filtered_word))
plt.imshow(samsung_word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

xiaomi_filtered_word_freq = FreqDist(xiaomi_filtered_word)
xiaomi_filtered_word_freq.most_common(20)

xiaomi_word_cloud = WordCloud(width = 3000, height = 2000, random_state=1, max_words=30, background_color='black', \
                            colormap='Set2', collocations=False).generate(str(xiaomi_filtered_word))
plt.imshow(xiaomi_word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# 19. Reconocimiento entidad nombrada
Luego de identificar las palabras con mayor nivel de ocurrencia, continuaremos con la exploración de la información para tratar de resolver nuestra pregunta de investigación, con este fin implementaremos un analisis de reconocimiento de la entidad nombrada.

El objetivo principal del etiquetado de parte del discurso (POS) es identificar el grupo gramatical de una palabra determinada. Ya sea SUSTANTIVO, PRONOMBRE, ADJETIVO, VERBO, ADVERVIO, etc. según el contexto en el que se presenta.

![image-2.png](attachment:image-2.png)imagen.png

Por lo que es fundamental trabajar con la frases originales, sin mayor edición, para garantizar la comprensión del contexto. El etiquetado de POS Tagging asigna la categoria o etiqueta correspondiente a cada palabra de la frase. A continuación se presenta el significado de cada una de las etiquetas:

![image.png](attachment:image.png)imagen-2.png

Teniendo en cuenta que nos interesa identificar asuntos asociados con la forma en que estas Universidades comunican la sostenibilidad, vamos a priorizar la identificación de sustativos (NN) y adjetivos (JJ), para identificar aquellos con mayor nivel de ocurrencia.

Pastel: https://stackoverflow.com/questions/49355114/tagging-a-txt-file-from-inaugural-address-corpus

https://stackoverflow.com/questions/21882460/filter-specific-part-of-speech-nltk

https://stackoverflow.com/questions/33553046/storing-values-from-loop-in-a-list-or-tuple-in-python/33553178
"""

#open(r"scraped_data_20211005_e.txt", encoding="utf-8").read().replace("'',", "")

huawei_tagged_token = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = huawei
for i in range(0, len(lines)):
    token_text = word_tokenize(lines[i].lower().replace("á", "a").replace("é", "e").replace("í", "i").\
                                   replace("ó", "o").replace("ú", "u"))
    huawei_tagged_token += nltk.pos_tag(token_text)

#open(r"scraped_data_20211005_e.txt", encoding="utf-8").read().replace("'',", "")

motorola_tagged_token = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = motorola
for i in range(0, len(lines)):
    token_text = word_tokenize(lines[i].lower().replace("á", "a").replace("é", "e").replace("í", "i").\
                                   replace("ó", "o").replace("ú", "u"))
    motorola_tagged_token += nltk.pos_tag(token_text)

#open(r"scraped_data_20211005_e.txt", encoding="utf-8").read().replace("'',", "")

samsung_tagged_token = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = samsung
for i in range(0, len(lines)):
    token_text = word_tokenize(lines[i].lower().replace("á", "a").replace("é", "e").replace("í", "i").\
                                   replace("ó", "o").replace("ú", "u"))
    samsung_tagged_token += nltk.pos_tag(token_text)

#open(r"scraped_data_20211005_e.txt", encoding="utf-8").read().replace("'',", "")

xiaomi_tagged_token = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = xiaomi
for i in range(0, len(lines)):
    token_text = word_tokenize(lines[i].lower().replace("á", "a").replace("é", "e").replace("í", "i").\
                                   replace("ó", "o").replace("ú", "u"))
    xiaomi_tagged_token += nltk.pos_tag(token_text)

"""Como estamos trabajando con la base de datos original, es nesamsungario limpiar más los tokens, por lo que retiraremos las palabras vacias (stopwords) y otras palabras que detectamos como innesamsungarias, para identificar aquellos sustantivos y adjetivos que mayor ocurrencia tienen."""

stop_words=set(stopwords.words("spanish"))
print(stop_words)

palbras_inn = {'ver', 'mas','https','http'}

nn_huawei_tags = [t for t in huawei_tagged_token if t[1] == "NN"]
nn_huawei_tags_1 = [t for t in nn_huawei_tags if t[0] not in stopwords.words("spanish")]
nn_huawei_tags_2 = [t for t in nn_huawei_tags_1 if t[0] not in palbras_inn]

nn_huawei_tags_freq = FreqDist(nn_huawei_tags_2)
nn_huawei_tags_freq.most_common(30)

jj_huawei_tags = [t for t in huawei_tagged_token if t[1] == "JJ"]
jj_huawei_tags_1 = [t for t in jj_huawei_tags if t[0] not in stopwords.words("spanish")]
jj_huawei_tags_2 = [t for t in jj_huawei_tags_1 if t[0] not in palbras_inn]

jj_huawei_tags_freq = FreqDist(jj_huawei_tags_2)
jj_huawei_tags_freq.most_common(30)

nn_motorola_tags = [t for t in motorola_tagged_token if t[1] == "NN"]
nn_motorola_tags_1 = [t for t in nn_motorola_tags if t[0] not in stopwords.words("spanish")]
nn_motorola_tags_2 = [t for t in nn_motorola_tags_1 if t[0] not in palbras_inn]

nn_motorola_tags_freq = FreqDist(nn_motorola_tags_2)
nn_motorola_tags_freq.most_common(30)

jj_motorola_tags = [t for t in motorola_tagged_token if t[1] == "JJ"]
jj_motorola_tags_1 = [t for t in jj_motorola_tags if t[0] not in stopwords.words("spanish")]
jj_motorola_tags_2 = [t for t in jj_motorola_tags_1 if t[0] not in palbras_inn]

jj_motorola_tags_freq = FreqDist(jj_motorola_tags_2)
jj_motorola_tags_freq.most_common(30)

nn_samsung_tags = [t for t in samsung_tagged_token if t[1] == "NN"]
nn_samsung_tags_1 = [t for t in nn_samsung_tags if t[0] not in stopwords.words("spanish")]
nn_samsung_tags_2 = [t for t in nn_samsung_tags_1 if t[0] not in palbras_inn]

nn_samsung_tags_freq = FreqDist(nn_samsung_tags_2)
nn_samsung_tags_freq.most_common(30)

jj_samsung_tags = [t for t in samsung_tagged_token if t[1] == "JJ"]
jj_samsung_tags_1 = [t for t in jj_samsung_tags if t[0] not in stopwords.words("spanish")]
jj_samsung_tags_2 = [t for t in jj_samsung_tags_1 if t[0] not in palbras_inn]

jj_samsung_tags_freq = FreqDist(jj_samsung_tags_2)
jj_samsung_tags_freq.most_common(30)

nn_xiaomi_tags = [t for t in xiaomi_tagged_token if t[1] == "NN"]
nn_xiaomi_tags_1 = [t for t in nn_xiaomi_tags if t[0] not in stopwords.words("spanish")]
nn_xiaomi_tags_2 = [t for t in nn_xiaomi_tags_1 if t[0] not in palbras_inn]

nn_xiaomi_tags_freq = FreqDist(nn_xiaomi_tags_2)
nn_xiaomi_tags_freq.most_common(30)

jj_xiaomi_tags = [t for t in xiaomi_tagged_token if t[1] == "JJ"]
jj_xiaomi_tags_1 = [t for t in jj_xiaomi_tags if t[0] not in stopwords.words("spanish")]
jj_xiaomi_tags_2 = [t for t in jj_xiaomi_tags_1 if t[0] not in palbras_inn]

jj_xiaomi_tags_freq = FreqDist(jj_xiaomi_tags_2)
jj_xiaomi_tags_freq.most_common(30)

"""20. Distribución de frecuencias
Luego de tener el reconocimiento de la entidad nombrada, a continuación revisaremos la distribución de frecuencias para identificar aquellas palabras que presentan una mayor ocurrencia, es decir los sustantivos y adjetivos más populares en las comunicaciones de cada universidad.
"""

nn_huawei_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

jj_huawei_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

nn_motorola_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

jj_motorola_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

nn_samsung_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

jj_samsung_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

nn_xiaomi_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

jj_xiaomi_tags_freq.plot(30,cumulative=False)
plt.figure(figsize = (50,50))
plt.show()

"""# 21. Análisis de sentimientos¶
Tras completar el segundo análisis exploratorio de la información, a continuación realizaremos un analisis de sentimientos para tratar de identificar el tono general -i.e., Netro, positivo o negativo- de cada uno de los post que hacen las universidades.

Pastel: https://datascience.stackexchange.com/questions/6691/sentiment-analysis-model-for-spanish

https://stackoverflow.com/questions/8899905/count-number-of-occurrensamsung-of-a-substring-in-a-string
"""

from pysentimiento import create_analyzer
analyzer = create_analyzer(task="sentiment", lang="es")

huawei_sentiment = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = huawei
for i in lines:
    sentiment_huawei = analyzer.predict(i)
    huawei_sentiment.append(sentiment_huawei)

huawei_sentiment_string = str(huawei_sentiment)
huawei_sentiment_string_POS = huawei_sentiment_string.count("output=POS")
huawei_sentiment_string_NEU = huawei_sentiment_string.count("output=NEU")
huawei_sentiment_string_NEG = huawei_sentiment_string.count("output=NEG")

sentimiento = ['POS','NEU','NEG']
valor = [huawei_sentiment_string_POS,huawei_sentiment_string_NEU,huawei_sentiment_string_NEG]
semaforo = ['green', 'yellow', 'red']

plt.bar(sentimiento, valor, color=semaforo)
plt.title('Análisis de sentimientos')
plt.xlabel('Positivos, neutros y negativos')
plt.ylabel('Puntuación')
plt.show()

motorola_sentiment = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = motorola
for i in lines:
    sentiment_motorola = analyzer.predict(i)
    motorola_sentiment.append(sentiment_motorola)

motorola_sentiment_string = str(motorola_sentiment)
motorola_sentiment_string_POS = motorola_sentiment_string.count("output=POS")
motorola_sentiment_string_NEU = motorola_sentiment_string.count("output=NEU")
motorola_sentiment_string_NEG = motorola_sentiment_string.count("output=NEG")

sentimiento = ['POS','NEU','NEG']
valor = [motorola_sentiment_string_POS,motorola_sentiment_string_NEU,motorola_sentiment_string_NEG]
semaforo = ['green', 'yellow', 'red']

plt.bar(sentimiento, valor, color=semaforo)
plt.title('Análisis de sentimientos')
plt.xlabel('Positivos, neutros y negativos')
plt.ylabel('Puntuación')
plt.show()

samsung_sentiment = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = samsung
for i in lines:
    sentiment_samsung = analyzer.predict(i)
    samsung_sentiment.append(sentiment_samsung)

samsung_sentiment_string = str(samsung_sentiment)
samsung_sentiment_string_POS = samsung_sentiment_string.count("output=POS")
samsung_sentiment_string_NEU = samsung_sentiment_string.count("output=NEU")
samsung_sentiment_string_NEG = samsung_sentiment_string.count("output=NEG")

sentimiento = ['POS','NEU','NEG']
valor = [samsung_sentiment_string_POS,samsung_sentiment_string_NEU,samsung_sentiment_string_NEG]
semaforo = ['green', 'yellow', 'red']

plt.bar(sentimiento, valor, color=semaforo)
plt.title('Análisis de sentimientos')
plt.xlabel('Positivos, neutros y negativos')
plt.ylabel('Puntuación')
plt.show()

xiaomi_sentiment = []
#with open(r"scraped_data_20211005_e.txt", encoding="utf-8") as f:
lines = xiaomi
for i in lines:
    sentiment_xiaomi = analyzer.predict(i)
    xiaomi_sentiment.append(sentiment_xiaomi)

xiaomi_sentiment_string = str(xiaomi_sentiment)
xiaomi_sentiment_string_POS = xiaomi_sentiment_string.count("output=POS")
xiaomi_sentiment_string_NEU = xiaomi_sentiment_string.count("output=NEU")
xiaomi_sentiment_string_NEG = xiaomi_sentiment_string.count("output=NEG")

sentimiento = ['POS','NEU','NEG']
valor = [xiaomi_sentiment_string_POS,xiaomi_sentiment_string_NEU,xiaomi_sentiment_string_NEG]
semaforo = ['green', 'yellow', 'red']

plt.bar(sentimiento, valor, color=semaforo)
plt.title('Análisis de sentimientos')
plt.xlabel('Positivos, neutros y negativos')
plt.ylabel('Puntuación')
plt.show()

"""# 22. Consolidar base de datos para análisis de regresión¶
A continuación se consolida la base de datos para el posterior analisis de regresión, las fases en las que se aplica este analisis son:

1.Consolidar las bases de datos de las 5 universidades
2.Seleccionar elementos únicos (pastel: https://stackoverflow.com/questions/43184491/df-unique-on-whole-dataframe-based-on-a-column)
3.Ejecutar el analisis de sentimientos a la base de datos consolidada (pastel: https://stackoverflow.com/questions/68416974/convert-string-to-colum)
4.Crear una variable en donde se haga referencia a la sostenibilidad (pastel: https://stackoverflow.com/questions/36701689/assign-value-to-a-pandas-dataframe-column-based-on-string-condition)
5.Seleccionar las columnas de interes

Consolidar las bases de datos de las 5 universidades
"""

import pandas as pd
import glob

path = r'C:\Users\ESTUDIANTE\Scrap Twitter\twint\data\all'
all_files = glob.glob(path + "/*.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

all_HEIS = pd.concat(li, axis=0, ignore_index=True)

"""Seleccionar elementos únicos (pastel: https://stackoverflow.com/questions/43184491/df-unique-on-whole-dataframe-based-on-a-column)"""

all_HEIS_unique = all_HEIS.drop_duplicates(subset=['tweet'])
all_HEIS_unique.head(2)

"""Ejecutar el analisis de sentimientos a la base de datos consolidada (pastel: https://stackoverflow.com/questions/68416974/convert-string-to-colum)"""

#analyzer = SentimentAnalyzer(lang="es")


def prosamsungs(row):
    res = analyzer.predict(row["tweet"])
    return pd.Series({'sentiment': res.output, **res.probas})


all_HEIS_unique_sentiment = all_HEIS_unique.join(all_HEIS.apply(prosamsungs, axis=1))

"""Crear una variable en donde se haga referencia a la sostenibilidad (pastel: https://stackoverflow.com/questions/36701689/assign-value-to-a-pandas-dataframe-column-based-on-string-condition)"""

all_HEIS_unique_sentiment.loc[all_HEIS_unique_sentiment['tweet'].str.contains("mejor"),'post_mejor'] = '1'

all_HEIS_unique_sentiment.head(2)

"""Seleccionar las columnas de interes"""

all_HEIS_unique_sentiment_filter = all_HEIS_unique_sentiment[["id", "time", "username", "mentions", "urls", "photos", "replies_count", "retweets_count", "likes_count", "hashtags", "video", "reply_to", "sentiment", "NEG", "NEU", "POS", "post_mejor"]]
all_HEIS_unique_sentiment_filter.head(2)

"""# 23. Transformación de variables¶
A continuacion, se verifica cada una de las variables de interes, y se tranforman aquellas que no se ajustan a los esquemas de regresión de interes.
"""

all_HEIS_unique_sentiment_filter.info()

"""Convertir la columna "time" en formato de tiempo, para extraer la hora del día en que se hace el post"""

all_HEIS_filter_hour = pd.DataFrame()
all_HEIS_filter_hour['time'] = pd.to_datetime(all_HEIS_unique_sentiment_filter['time'], format='%H:%M:%S')
all_HEIS_filter_hour.info()

all_HEIS_unique_sentiment_filter["post_hour"] = all_HEIS_filter_hour['time'].dt.strftime('%H').astype(str).astype(int)
display(all_HEIS_unique_sentiment_filter)

"""Transformar las variables: mentions, urls, photos, hashtags, reply_to a variables dicotomicas.
Pastel: https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column
"""

all_HEIS_unique_sentiment_filter.describe()

all_HEIS_unique_sentiment_filter['post_mentions'] = np.where(all_HEIS_unique_sentiment_filter['mentions']=='[]', '0', '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['post_urls'] = np.where(all_HEIS_unique_sentiment_filter['urls']=='[]', '0', '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['post_photos'] = np.where(all_HEIS_unique_sentiment_filter['photos']=='[]', '0', '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['post_hashtags'] = np.where(all_HEIS_unique_sentiment_filter['hashtags']=='[]', '0', '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['post_reply_to'] = np.where(all_HEIS_unique_sentiment_filter['reply_to']=='[]', '0', '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['post_mejor'] = all_HEIS_unique_sentiment_filter['post_mejor'].fillna(0).astype(int)

all_HEIS_unique_sentiment_filter['replies_count_bin'] = all_HEIS_unique_sentiment_filter.replies_count.map( lambda x: '0' if x <= 2 else '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['retweets_count_bin'] = all_HEIS_unique_sentiment_filter.retweets_count.map( lambda x: '0' if x <= 11 else '1').astype(str).astype(int)
all_HEIS_unique_sentiment_filter['likes_count_bin'] = all_HEIS_unique_sentiment_filter.likes_count.map( lambda x: '0' if x <= 47 else '1').astype(str).astype(int)


display(all_HEIS_unique_sentiment_filter)

all_HEIS_unique_sentiment_filter[all_HEIS_unique_sentiment_filter['replies_count_bin'] == 1]

all_HEIS_unique_sentiment_filter.info()

"""# 24. Separar los dataframe por universidad
Para hacer el posterior analisis de regresion, se dividiran el dataframe actual en varios, uno para cada universidad
"""

all_HEIS_unique_sentiment_filter = all_HEIS_unique_sentiment_filter[["id", "username", "replies_count_bin", "retweets_count_bin", "likes_count_bin", "video", "NEG", "NEU", "POS", "post_mejor", "post_hour", "post_mentions", "post_urls", "post_photos", "post_hashtags", "post_reply_to"]]
display(all_HEIS_unique_sentiment_filter)

huawei_complete = all_HEIS_unique_sentiment_filter.loc[(all_HEIS_unique_sentiment_filter['username'] == 'huaweimobileco')]
motorola_complete = all_HEIS_unique_sentiment_filter.loc[(all_HEIS_unique_sentiment_filter['username'] == 'motorola_col')]
samsung_complete = all_HEIS_unique_sentiment_filter.loc[(all_HEIS_unique_sentiment_filter['username'] == 'samsungmobileco')]
xiaomi_complete = all_HEIS_unique_sentiment_filter.loc[(all_HEIS_unique_sentiment_filter['username'] == 'xiaomicolombia')]

huawei_complete.info()

motorola_complete.info()

samsung_complete.info()

xiaomi_complete.info()

"""# 25. Diferentes modelos de regresión¶
A continuación se plantean diferentes modelos de regresión para identificar la incidencia de los elementos internos del mensaje posteado, con las reacciones que se generan a nivel externo entre los usuarios de la red social:

Elementos internos del mensaje:
video
NEG
NEU
POS
post_hour
post_mentions
post_urls
post_photos
post_hashtags
post_reply_to
post_mejor
Elementos externos:
replies_count_bin
retweets_count_bin
likes_count_bin
"""

huawei_complete.head(2)

"""Se revisa la existencia de posibles correlaciones entre las variables no categoricas que se incluirán en el modelo"""

to_drop = ["id", "username"] #Las que son categoricas
df_correlation = huawei_complete.drop(to_drop, axis = 1).corr()

mask = np.triu(np.ones_like(df_correlation, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(df_correlation, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

to_drop = ["id", "username"] #Las que son categoricas
df_correlation = motorola_complete.drop(to_drop, axis = 1).corr()

mask = np.triu(np.ones_like(df_correlation, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(df_correlation, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

to_drop = ["id", "username"] #Las que son categoricas
df_correlation = samsung_complete.drop(to_drop, axis = 1).corr()

mask = np.triu(np.ones_like(df_correlation, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(df_correlation, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

to_drop = ["id", "username"] #Las que son categoricas
df_correlation = xiaomi_complete.drop(to_drop, axis = 1).corr()

mask = np.triu(np.ones_like(df_correlation, dtype=bool))
f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(230, 20, as_cmap=True)

sns.heatmap(df_correlation, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)

"""# 25.1 Primera prueba con variable "replies_count_bin" como variable dependiente para la huawei¶
A continuación vamos a hacer la verificacon del modelo con la variable "replies_count_bin" como variable dependiente
"""

to_drop = ["id", "username", "retweets_count_bin", "likes_count_bin"] #Las que son categoricas
huawei_complete = huawei_complete.drop(to_drop, axis = 1)
display(huawei_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(huawei_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
#supvm = svm.SVC()

df_desertor = huawei_complete[huawei_complete['replies_count_bin'] == 1]
df_no_desertor = huawei_complete[huawei_complete['replies_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['replies_count_bin'].values
X1 = df_new.drop('replies_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['replies_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.2 Primera prueba con variable "retweets_count_bin" como variable dependiente para la huawei¶
A continuación vamos a hacer la verificacon del modelo con la variable "retweets_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "likes_count_bin"] #Las que son categoricas
huawei_complete = huawei_complete.drop(to_drop, axis = 1)
display(huawei_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(huawei_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = huawei_complete[huawei_complete['retweets_count_bin'] == 1]
df_no_desertor = huawei_complete[huawei_complete['retweets_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['retweets_count_bin'].values
X1 = df_new.drop('retweets_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['retweets_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.3 Primera prueba con variable "likes_count_bin" como variable dependiente para la huawei
A continuación vamos a hacer la verificacon del modelo con la variable "likes_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "retweets_count_bin"] #Las que son categoricas
huawei_complete = huawei_complete.drop(to_drop, axis = 1)
display(huawei_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(huawei_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = huawei_complete[huawei_complete['likes_count_bin'] == 1]
df_no_desertor = huawei_complete[huawei_complete['likes_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor), replace=True) #Le agregue el replace=True

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['likes_count_bin'].values
X1 = df_new.drop('likes_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['likes_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.1 Primera prueba con variable "replies_count_bin" como variable dependiente para la motorola¶
A continuación vamos a hacer la verificacon del modelo con la variable "replies_count_bin" como variable dependiente
"""

to_drop = ["id", "username", "retweets_count_bin", "likes_count_bin"] #Las que son categoricas
motorola_complete = motorola_complete.drop(to_drop, axis = 1)
display(motorola_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(motorola_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
#supvm = svm.SVC()

df_desertor = motorola_complete[motorola_complete['replies_count_bin'] == 1]
df_no_desertor = motorola_complete[motorola_complete['replies_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['replies_count_bin'].values
X1 = df_new.drop('replies_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['replies_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.2 Primera prueba con variable "retweets_count_bin" como variable dependiente para la motorola¶
A continuación vamos a hacer la verificacon del modelo con la variable "retweets_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "likes_count_bin"] #Las que son categoricas
motorola_complete = motorola_complete.drop(to_drop, axis = 1)
display(motorola_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(motorola_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = motorola_complete[motorola_complete['retweets_count_bin'] == 1]
df_no_desertor = motorola_complete[motorola_complete['retweets_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['retweets_count_bin'].values
X1 = df_new.drop('retweets_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['retweets_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "retweets_count_bin"] #Las que son categoricas
motorola_complete = motorola_complete.drop(to_drop, axis = 1)
display(motorola_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(motorola_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = motorola_complete[motorola_complete['likes_count_bin'] == 1]
df_no_desertor = motorola_complete[motorola_complete['likes_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor), replace=True) #Le agregue el replace=True

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['likes_count_bin'].values
X1 = df_new.drop('likes_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['likes_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.1 Primera prueba con variable "replies_count_bin" como variable dependiente para la samsung¶
A continuación vamos a hacer la verificacon del modelo con la variable "replies_count_bin" como variable dependiente
"""

to_drop = ["id", "username", "retweets_count_bin", "likes_count_bin"] #Las que son categoricas
samsung_complete = samsung_complete.drop(to_drop, axis = 1)
display(samsung_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(samsung_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
#supvm = svm.SVC()

df_desertor = samsung_complete[samsung_complete['replies_count_bin'] == 1]
df_no_desertor = samsung_complete[samsung_complete['replies_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['replies_count_bin'].values
X1 = df_new.drop('replies_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['replies_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.2 Primera prueba con variable "retweets_count_bin" como variable dependiente para la samsung¶
A continuación vamos a hacer la verificacon del modelo con la variable "retweets_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "likes_count_bin"] #Las que son categoricas
samsung_complete = samsung_complete.drop(to_drop, axis = 1)
display(samsung_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(samsung_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = samsung_complete[samsung_complete['retweets_count_bin'] == 1]
df_no_desertor = samsung_complete[samsung_complete['retweets_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['retweets_count_bin'].values
X1 = df_new.drop('retweets_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['retweets_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.3 Primera prueba con variable "likes_count_bin" como variable dependiente para la samsung
A continuación vamos a hacer la verificacon del modelo con la variable "likes_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "retweets_count_bin"] #Las que son categoricas
samsung_complete = samsung_complete.drop(to_drop, axis = 1)
display(samsung_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(samsung_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = samsung_complete[samsung_complete['likes_count_bin'] == 1]
df_no_desertor = samsung_complete[samsung_complete['likes_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor), replace=True) #Le agregue el replace=True

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['likes_count_bin'].values
X1 = df_new.drop('likes_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['likes_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.1 Primera prueba con variable "replies_count_bin" como variable dependiente para la xiaomi¶
A continuación vamos a hacer la verificacon del modelo con la variable "replies_count_bin" como variable dependiente
"""

to_drop = ["id", "username", "retweets_count_bin", "likes_count_bin"] #Las que son categoricas
xiaomi_complete = xiaomi_complete.drop(to_drop, axis = 1)
display(xiaomi_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(xiaomi_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
#supvm = svm.SVC()

df_desertor = xiaomi_complete[xiaomi_complete['replies_count_bin'] == 1]
df_no_desertor = xiaomi_complete[xiaomi_complete['replies_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['replies_count_bin'].values
X1 = df_new.drop('replies_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['replies_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.2 Primera prueba con variable "retweets_count_bin" como variable dependiente para la xiaomi¶
A continuación vamos a hacer la verificacon del modelo con la variable "retweets_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "likes_count_bin"] #Las que son categoricas
xiaomi_complete = xiaomi_complete.drop(to_drop, axis = 1)
display(xiaomi_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(xiaomi_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = xiaomi_complete[xiaomi_complete['retweets_count_bin'] == 1]
df_no_desertor = xiaomi_complete[xiaomi_complete['retweets_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor))

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['retweets_count_bin'].values
X1 = df_new.drop('retweets_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['retweets_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# 25.3 Primera prueba con variable "likes_count_bin" como variable dependiente para la xiaomi
A continuación vamos a hacer la verificacon del modelo con la variable "likes_count_bin" como variable dependiente
"""

#Volver a correr numeral 24 para que la BD se genere completa

to_drop = ["id", "username", "replies_count_bin", "retweets_count_bin"] #Las que son categoricas
xiaomi_complete = xiaomi_complete.drop(to_drop, axis = 1)
display(xiaomi_complete)

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn import svm
#Import ROC AUC score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_roc_curve

SEED = 1111
lr = LogisticRegression(random_state = SEED)
knn = KNN(n_neighbors = 6)
dt = DecisionTreeClassifier(criterion = 'gini', random_state = SEED, max_depth = round(len(xiaomi_complete)**(1/2)))
rfc = RandomForestClassifier(n_estimators = 500 , min_samples_leaf = 0.12, random_state =SEED)
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=100, random_state=1)
supvm = svm.SVC()

df_desertor = xiaomi_complete[xiaomi_complete['likes_count_bin'] == 1]
df_no_desertor = xiaomi_complete[xiaomi_complete['likes_count_bin'] == 0]

df_no_desertor = df_no_desertor.sample(n=len(df_desertor), replace=True) #Le agregue el replace=True

df_new = pd.concat([df_desertor, df_no_desertor])
df_new = df_new.sample(frac=1)

y1 = df_new['likes_count_bin'].values
X1 = df_new.drop('likes_count_bin', axis = 1).values
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.26, random_state = SEED, stratify = y1)

classifiers =[('Logistic Regression', lr), ('K Nearest Neighbors', knn),\
              ('Classification Tree', dt), ('Random Forest', rfc), ('Ada Boost', ada)]

vc = VotingClassifier(estimators = classifiers)

for clf_name, clf in classifiers:
    clf.fit(X1_train, y1_train)
    y1_pred = clf.predict(X1_test)

    y_pred_proba = clf.predict_proba(X1_test)[:,1]
    roc_score = roc_auc_score(y1_test, y_pred_proba)

    print('{:s} accuracy: {:.3f}'.format(clf_name, accuracy_score(y1_test, y1_pred)))
    print('{:s} ROC score: {:.3f}'.format(clf_name, roc_score))
    print('')

vc.fit(X1_train, y1_train)
y1_predVC = vc.predict(X1_test)
print('Voting Classifier accuracy: {:.3f}'.format(accuracy_score(y1_test, y1_predVC)))

sns.set()
fig, axes = plt.subplots(2, 3, figsize = (20,12))
idxs = [axes[0,0], axes[0,1], axes[0,2], axes[1,0], axes[1,1]]

for (clf_name, clf), idx in zip(classifiers, idxs):
    #clf.fit(X1_train, y1_train)
    plot_roc_curve(clf, X1_test, y1_test, ax = idx)

#Feature importance
fig, axes = plt.subplots(figsize= (12,7))
importances_rf =pd.Series(rfc.feature_importances_[0:15], index = df_new.drop\
                                          (['likes_count_bin'], axis = 1).columns[0:15])
sorted_importances_rf = importances_rf.sort_values()
sns.set()
sorted_importances_rf.plot(kind='barh', color = 'lightblue')
plt.show()

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
print(len(max_depth))
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, cv = 3, n_iter = 20, n_jobs = -1,\
                               random_state=SEED)

rf_random.fit(X1_train, y1_train)

print(rf_random.best_params_)
best_random_model = rf_random.best_estimator_
yrf_pred_random = best_random_model.predict(X1_test)
print('Random Forest accuracy: {:.3f}'.format(accuracy_score(y1_test, yrf_pred_random)))

#Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

rf_with_best_params = RandomForestClassifier(n_estimators = 733 , min_samples_split = 2, random_state =SEED,\
                    min_samples_leaf = 1, max_features = 'sqrt', max_depth = 70, bootstrap = False)

clf_names = ['Logistic Regression', 'K Nearest Neighbors', 'Classification Tree', 'Random Forest', 'Ada Boost']
clfs = [lr, knn, dt, rfc, ada]


cross_val_strategy = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

for classifier, classifier_name in zip(clfs, clf_names):
    scores = cross_val_score(classifier, X1_train, y1_train, cv = cross_val_strategy)

    mean_acc = np.mean(scores)
    std_acc = np.std(scores)

    print('{:s}  Mean accuracy: {:.3f}'.format(classifier_name, mean_acc))
    print('{:s}  Mean std: {:.3f}'.format(classifier_name, std_acc))
    print('')

"""# Ultima entrega algoritmo Maquina de Soporte de vectores"""

import numpy as np
import pandas as pd
from pydataset import data
from sklearn import svm
from sklearn import model_selection
from statsmodels.tools.eval_measures import mse

"""# Data preparation"""

huawei_complete.info()

huawei_complete.head()













# Data Manipulation
import pandas as pd # for data manipulation
import numpy as np # for data manipulation

# Sklearn
from sklearn.linear_model import LinearRegression # for building a linear regression model
from sklearn.svm import SVR # for building SVR model
from sklearn.preprocessing import MinMaxScaler

# Visualizations
import plotly.graph_objects as go # for data visualization
import plotly.express as px # for data visualization

"""# HUAWEI"""

huawei_complete.info()

# Use MinMax scaling on X2 and X3 features
scaler = MinMaxScaler()
huawei_complete['video (scaled)']=scaler.fit_transform(huawei_complete[['video']])
huawei_complete['post_hour (scaled)']=scaler.fit_transform(huawei_complete[['post_hour']])

# Print Dataframe
huawei_complete

# Create a 3D scatter plot
fig = px.scatter_3d(huawei_complete,
                    x=huawei_complete['post_hour (scaled)'],
                    y=huawei_complete['video (scaled)'],
                    z=huawei_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    height=900, width=1000
                   )

# Set figure title
fig.update_layout(title_text="Scatter 3D Plot",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))

# Update marker size
fig.update_traces(marker=dict(size=2))

fig.show()

# ----------- Select variables -----------
X=huawei_complete[['video (scaled)','post_hour (scaled)']]
y=huawei_complete['likes_count'].values

# ----------- Model fitting -----------
# Define models and set hyperparameter values
model1 = LinearRegression()
model2 = SVR(kernel='rbf', C=100, epsilon=1)

# Fit the two models
lr = model1.fit(X, y)
svr = model2.fit(X, y)

# ----------- For creating a prediciton plane to be used in the visualization -----------
# Set Increments between points in a meshgrid
mesh_size = 0.05

# Identify min and max values for input variables
x_min, x_max = X['video (scaled)'].min(), X['video (scaled)'].max()
y_min, y_max = X['post_hour (scaled)'].min(), X['post_hour (scaled)'].max()

# Return evenly spaced values based on a range between min and max
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)

# Create a meshgrid
xx, yy = np.meshgrid(xrange, yrange)

# ----------- Create a prediciton plane  -----------
# Use models to create a prediciton plane --- Linear Regression
pred_LR = model1.predict(np.c_[xx.ravel(), yy.ravel()])
pred_LR = pred_LR.reshape(xx.shape)

# Use models to create a prediciton plane --- SVR
pred_svr = model2.predict(np.c_[xx.ravel(), yy.ravel()])
pred_svr = pred_svr.reshape(xx.shape)

# Note, .ravel() flattens the array to a 1D array,
# then np.c_ takes elements from flattened xx and yy arrays and puts them together,
# this creates the right shape required for model input

# prediction array that is created by the model output is a 1D array,
# Hence, we need to reshape it to be the same shape as xx or yy to be able to display it on a graph

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(huawei_complete, x=huawei_complete['video (scaled)'], y=huawei_complete['post_hour (scaled)'], z=huawei_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with Linear Regression Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_LR, name='LR',
                          colorscale=px.colors.sequential.Sunsetdark, showscale=False))

fig.show()

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(huawei_complete, x=huawei_complete['post_hour (scaled)'], y=huawei_complete['video (scaled)'], z=huawei_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with SVR Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_svr, name='SVR',
                          colorscale=px.colors.sequential.Sunsetdark,
                          showscale=False))

fig.show()

"""# MOTOROLA"""

motorola_complete.info()

# Use MinMax scaling on X2 and X3 features
scaler = MinMaxScaler()
motorola_complete['video (scaled)']=scaler.fit_transform(motorola_complete[['video']])
motorola_complete['post_hour (scaled)']=scaler.fit_transform(motorola_complete[['post_hour']])

# Print Dataframe
motorola_complete

# Create a 3D scatter plot
fig = px.scatter_3d(motorola_complete,
                    x=motorola_complete['post_hour (scaled)'],
                    y=motorola_complete['video (scaled)'],
                    z=motorola_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    height=900, width=1000
                   )

# Set figure title
fig.update_layout(title_text="Scatter 3D Plot",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))

# Update marker size
fig.update_traces(marker=dict(size=2))

fig.show()

# ----------- Select variables -----------
X=motorola_complete[['video (scaled)','post_hour (scaled)']]
y=motorola_complete['likes_count'].values

# ----------- Model fitting -----------
# Define models and set hyperparameter values
model1 = LinearRegression()
model2 = SVR(kernel='rbf', C=100, epsilon=1)

# Fit the two models
lr = model1.fit(X, y)
svr = model2.fit(X, y)

# ----------- For creating a prediciton plane to be used in the visualization -----------
# Set Increments between points in a meshgrid
mesh_size = 0.05

# Identify min and max values for input variables
x_min, x_max = X['video (scaled)'].min(), X['video (scaled)'].max()
y_min, y_max = X['post_hour (scaled)'].min(), X['post_hour (scaled)'].max()

# Return evenly spaced values based on a range between min and max
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)

# Create a meshgrid
xx, yy = np.meshgrid(xrange, yrange)

# ----------- Create a prediciton plane  -----------
# Use models to create a prediciton plane --- Linear Regression
pred_LR = model1.predict(np.c_[xx.ravel(), yy.ravel()])
pred_LR = pred_LR.reshape(xx.shape)

# Use models to create a prediciton plane --- SVR
pred_svr = model2.predict(np.c_[xx.ravel(), yy.ravel()])
pred_svr = pred_svr.reshape(xx.shape)

# Note, .ravel() flattens the array to a 1D array,
# then np.c_ takes elements from flattened xx and yy arrays and puts them together,
# this creates the right shape required for model input

# prediction array that is created by the model output is a 1D array,
# Hence, we need to reshape it to be the same shape as xx or yy to be able to display it on a graph

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(motorola_complete, x=motorola_complete['video (scaled)'], y=motorola_complete['post_hour (scaled)'], z=motorola_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with Linear Regression Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_LR, name='LR',
                          colorscale=px.colors.sequential.Sunsetdark, showscale=False))

fig.show()

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(motorola_complete, x=motorola_complete['post_hour (scaled)'], y=motorola_complete['video (scaled)'], z=motorola_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with SVR Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_svr, name='SVR',
                          colorscale=px.colors.sequential.Sunsetdark,
                          showscale=False))

fig.show()

"""# SAMSUNG"""

samsung_complete.info()

# Use MinMax scaling on X2 and X3 features
scaler = MinMaxScaler()
samsung_complete['video (scaled)']=scaler.fit_transform(samsung_complete[['video']])
samsung_complete['post_hour (scaled)']=scaler.fit_transform(samsung_complete[['post_hour']])

# Print Dataframe
samsung_complete

# Create a 3D scatter plot
fig = px.scatter_3d(samsung_complete,
                    x=samsung_complete['post_hour (scaled)'],
                    y=samsung_complete['video (scaled)'],
                    z=samsung_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    height=900, width=1000
                   )

# Set figure title
fig.update_layout(title_text="Scatter 3D Plot",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))

# Update marker size
fig.update_traces(marker=dict(size=2))

fig.show()

# ----------- Select variables -----------
X=samsung_complete[['video (scaled)','post_hour (scaled)']]
y=samsung_complete['likes_count'].values

# ----------- Model fitting -----------
# Define models and set hyperparameter values
model1 = LinearRegression()
model2 = SVR(kernel='rbf', C=100, epsilon=1)

# Fit the two models
lr = model1.fit(X, y)
svr = model2.fit(X, y)

# ----------- For creating a prediciton plane to be used in the visualization -----------
# Set Increments between points in a meshgrid
mesh_size = 0.05

# Identify min and max values for input variables
x_min, x_max = X['video (scaled)'].min(), X['video (scaled)'].max()
y_min, y_max = X['post_hour (scaled)'].min(), X['post_hour (scaled)'].max()

# Return evenly spaced values based on a range between min and max
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)

# Create a meshgrid
xx, yy = np.meshgrid(xrange, yrange)

# ----------- Create a prediciton plane  -----------
# Use models to create a prediciton plane --- Linear Regression
pred_LR = model1.predict(np.c_[xx.ravel(), yy.ravel()])
pred_LR = pred_LR.reshape(xx.shape)

# Use models to create a prediciton plane --- SVR
pred_svr = model2.predict(np.c_[xx.ravel(), yy.ravel()])
pred_svr = pred_svr.reshape(xx.shape)

# Note, .ravel() flattens the array to a 1D array,
# then np.c_ takes elements from flattened xx and yy arrays and puts them together,
# this creates the right shape required for model input

# prediction array that is created by the model output is a 1D array,
# Hence, we need to reshape it to be the same shape as xx or yy to be able to display it on a graph

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(samsung_complete, x=samsung_complete['video (scaled)'], y=samsung_complete['post_hour (scaled)'], z=samsung_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with Linear Regression Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_LR, name='LR',
                          colorscale=px.colors.sequential.Sunsetdark, showscale=False))

fig.show()

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(samsung_complete, x=samsung_complete['post_hour (scaled)'], y=samsung_complete['video (scaled)'], z=samsung_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with SVR Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_svr, name='SVR',
                          colorscale=px.colors.sequential.Sunsetdark,
                          showscale=False))

fig.show()

"""# XIAOMI"""

xiaomi_complete.info()

# Use MinMax scaling on X2 and X3 features
scaler = MinMaxScaler()
xiaomi_complete['video (scaled)']=scaler.fit_transform(xiaomi_complete[['video']])
xiaomi_complete['post_hour (scaled)']=scaler.fit_transform(xiaomi_complete[['post_hour']])

# Print Dataframe
xiaomi_complete

# Create a 3D scatter plot
fig = px.scatter_3d(xiaomi_complete,
                    x=xiaomi_complete['post_hour (scaled)'],
                    y=xiaomi_complete['video (scaled)'],
                    z=xiaomi_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    height=900, width=1000
                   )

# Set figure title
fig.update_layout(title_text="Scatter 3D Plot",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))

# Update marker size
fig.update_traces(marker=dict(size=2))

fig.show()

# ----------- Select variables -----------
X=xiaomi_complete[['video (scaled)','post_hour (scaled)']]
y=xiaomi_complete['likes_count'].values

# ----------- Model fitting -----------
# Define models and set hyperparameter values
model1 = LinearRegression()
model2 = SVR(kernel='rbf', C=100, epsilon=1)

# Fit the two models
lr = model1.fit(X, y)
svr = model2.fit(X, y)

# ----------- For creating a prediciton plane to be used in the visualization -----------
# Set Increments between points in a meshgrid
mesh_size = 0.05

# Identify min and max values for input variables
x_min, x_max = X['video (scaled)'].min(), X['video (scaled)'].max()
y_min, y_max = X['post_hour (scaled)'].min(), X['post_hour (scaled)'].max()

# Return evenly spaced values based on a range between min and max
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)

# Create a meshgrid
xx, yy = np.meshgrid(xrange, yrange)

# ----------- Create a prediciton plane  -----------
# Use models to create a prediciton plane --- Linear Regression
pred_LR = model1.predict(np.c_[xx.ravel(), yy.ravel()])
pred_LR = pred_LR.reshape(xx.shape)

# Use models to create a prediciton plane --- SVR
pred_svr = model2.predict(np.c_[xx.ravel(), yy.ravel()])
pred_svr = pred_svr.reshape(xx.shape)

# Note, .ravel() flattens the array to a 1D array,
# then np.c_ takes elements from flattened xx and yy arrays and puts them together,
# this creates the right shape required for model input

# prediction array that is created by the model output is a 1D array,
# Hence, we need to reshape it to be the same shape as xx or yy to be able to display it on a graph

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(xiaomi_complete, x=xiaomi_complete['video (scaled)'], y=xiaomi_complete['post_hour (scaled)'], z=xiaomi_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with Linear Regression Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_LR, name='LR',
                          colorscale=px.colors.sequential.Sunsetdark, showscale=False))

fig.show()

# Create a 3D scatter plot with predictions
fig = px.scatter_3d(xiaomi_complete, x=xiaomi_complete['post_hour (scaled)'], y=xiaomi_complete['video (scaled)'], z=xiaomi_complete['likes_count'],
                    opacity=0.8, color_discrete_sequence=['black'],
                    width=1000, height=900
                   )

# Set figure title and colors
fig.update_layout(title_text="Scatter 3D Plot with SVR Prediction Surface",
                  scene_camera_eye=dict(x=1.5, y=1.5, z=0.25),
                  scene_camera_center=dict(x=0, y=0, z=-0.2),
                  scene = dict(xaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'),
                               yaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey'
                                          ),
                               zaxis=dict(backgroundcolor='white',
                                          color='black',
                                          gridcolor='lightgrey')))
# Update marker size
fig.update_traces(marker=dict(size=2))

# Add prediction plane
fig.add_traces(go.Surface(x=xrange, y=yrange, z=pred_svr, name='SVR',
                          colorscale=px.colors.sequential.Sunsetdark,
                          showscale=False))

fig.show()







import pandas as pd # for data manipulation
import numpy as np # for data manipulation

from sklearn.model_selection import train_test_split # for splitting the data into train and test samples
from sklearn.metrics import classification_report # for model evaluation metrics
from sklearn.preprocessing import OrdinalEncoder # for encoding categorical features from strings to number arrays

import plotly.express as px  # for data visualization
import plotly.graph_objects as go # for data visualization

# Differnt types of Naive Bayes Classifiers
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.naive_bayes import BernoulliNB

# Function that handles sample splitting, model fitting and report printing
def mfunc(X, y, typ):

    # Create training and testing samples
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    # Fit the model
    model = typ
    clf = model.fit(X_train, y_train)

    # Predict class labels on a test data
    pred_labels = model.predict(X_test)

    # Print model attributes
    print('Classes: ', clf.classes_) # class labels known to the classifier
    if str(typ)=='GaussianNB()':
        print('Class Priors: ',clf.class_prior_) # prior probability of each class.
    else:
        print('Class Log Priors: ',clf.class_log_prior_) # log prior probability of each class.

    # Use score method to get accuracy of the model
    print('--------------------------------------------------------')
    score = model.score(X_test, y_test)
    print('Accuracy Score: ', score)
    print('--------------------------------------------------------')

    # Look at classification report to evaluate the model
    print(classification_report(y_test, pred_labels))

    # Return relevant data for chart plotting
    return X_train, X_test, y_train, y_test, clf, pred_labels

"""# huawei"""

# Select data for modeling
X=huawei_complete[['video', 'post_hour']]
y=huawei_complete['likes_count'].values

# Fit the model and print the result
X_train, X_test, y_train, y_test, clf, pred_labels, = mfunc(X, y, GaussianNB())

# Specify a size of the mesh to be used
mesh_size = 5
margin = 1

# Create a mesh grid on which we will run our model
x_min, x_max = X.iloc[:, 0].fillna(X.mean()).min() - margin, X.iloc[:, 0].fillna(X.mean()).max() + margin
y_min, y_max = X.iloc[:, 1].fillna(X.mean()).min() - margin, X.iloc[:, 1].fillna(X.mean()).max() + margin
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)
xx, yy = np.meshgrid(xrange, yrange)

# Create classifier, run predictions on grid
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

# Specify traces
trace_specs = [
    #[X_train, y_train, 0, 'Train', 'brown'],
    #[X_train, y_train, 1, 'Train', 'aqua'],
    [X_test, y_test, 0, 'Test', 'red'],
    [X_test, y_test, 1, 'Test', 'blue']
]

# Build the graph using trace_specs from above
fig = go.Figure(data=[
    go.Scatter(
        x=X[y==label].iloc[:, 0], y=X[y==label].iloc[:, 1],
        name=f'{split} data, Actual Class: {label}',
        mode='markers', marker_color=marker
    )
    for X, y, label, split, marker in trace_specs
])

# Update marker size
fig.update_traces(marker_size=2, marker_line_width=0)

# Update axis range
fig.update_xaxes(range=[-100, 100])
fig.update_yaxes(range=[0,50])

# Update chart title and legend placement
fig.update_layout(title_text="Decision Boundary for Naive Bayes Model",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

# Add contour graph
fig.add_trace(
    go.Contour(
        x=xrange,
        y=yrange,
        z=Z,
        showscale=True,
        colorscale='magma',
        opacity=1,
        name='Score',
        hoverinfo='skip'
    )
)

fig.show()

"""# motorola"""

# Select data for modeling
X=motorola_complete[['video', 'post_hour']]
y=motorola_complete['likes_count'].values

# Fit the model and print the result
X_train, X_test, y_train, y_test, clf, pred_labels, = mfunc(X, y, GaussianNB())

# Specify a size of the mesh to be used
mesh_size = 5
margin = 1

# Create a mesh grid on which we will run our model
x_min, x_max = X.iloc[:, 0].fillna(X.mean()).min() - margin, X.iloc[:, 0].fillna(X.mean()).max() + margin
y_min, y_max = X.iloc[:, 1].fillna(X.mean()).min() - margin, X.iloc[:, 1].fillna(X.mean()).max() + margin
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)
xx, yy = np.meshgrid(xrange, yrange)

# Create classifier, run predictions on grid
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

# Specify traces
trace_specs = [
    #[X_train, y_train, 0, 'Train', 'brown'],
    #[X_train, y_train, 1, 'Train', 'aqua'],
    [X_test, y_test, 0, 'Test', 'red'],
    [X_test, y_test, 1, 'Test', 'blue']
]

# Build the graph using trace_specs from above
fig = go.Figure(data=[
    go.Scatter(
        x=X[y==label].iloc[:, 0], y=X[y==label].iloc[:, 1],
        name=f'{split} data, Actual Class: {label}',
        mode='markers', marker_color=marker
    )
    for X, y, label, split, marker in trace_specs
])

# Update marker size
fig.update_traces(marker_size=2, marker_line_width=0)

# Update axis range
fig.update_xaxes(range=[-100, 100])
fig.update_yaxes(range=[0,50])

# Update chart title and legend placement
fig.update_layout(title_text="Decision Boundary for Naive Bayes Model",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

# Add contour graph
fig.add_trace(
    go.Contour(
        x=xrange,
        y=yrange,
        z=Z,
        showscale=True,
        colorscale='magma',
        opacity=1,
        name='Score',
        hoverinfo='skip'
    )
)

fig.show()

"""# samsung

"""

# Select data for modeling
X=samsung_complete[['video', 'post_hour']]
y=samsung_complete['likes_count'].values

# Fit the model and print the result
X_train, X_test, y_train, y_test, clf, pred_labels, = mfunc(X, y, GaussianNB())

# Specify a size of the mesh to be used
mesh_size = 5
margin = 1

# Create a mesh grid on which we will run our model
x_min, x_max = X.iloc[:, 0].fillna(X.mean()).min() - margin, X.iloc[:, 0].fillna(X.mean()).max() + margin
y_min, y_max = X.iloc[:, 1].fillna(X.mean()).min() - margin, X.iloc[:, 1].fillna(X.mean()).max() + margin
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)
xx, yy = np.meshgrid(xrange, yrange)

# Create classifier, run predictions on grid
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

# Specify traces
trace_specs = [
    #[X_train, y_train, 0, 'Train', 'brown'],
    #[X_train, y_train, 1, 'Train', 'aqua'],
    [X_test, y_test, 0, 'Test', 'red'],
    [X_test, y_test, 1, 'Test', 'blue']
]

# Build the graph using trace_specs from above
fig = go.Figure(data=[
    go.Scatter(
        x=X[y==label].iloc[:, 0], y=X[y==label].iloc[:, 1],
        name=f'{split} data, Actual Class: {label}',
        mode='markers', marker_color=marker
    )
    for X, y, label, split, marker in trace_specs
])

# Update marker size
fig.update_traces(marker_size=2, marker_line_width=0)

# Update axis range
fig.update_xaxes(range=[-100, 100])
fig.update_yaxes(range=[0,50])

# Update chart title and legend placement
fig.update_layout(title_text="Decision Boundary for Naive Bayes Model",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

# Add contour graph
fig.add_trace(
    go.Contour(
        x=xrange,
        y=yrange,
        z=Z,
        showscale=True,
        colorscale='magma',
        opacity=1,
        name='Score',
        hoverinfo='skip'
    )
)

fig.show()

"""# xiaomi"""

# Select data for modeling
X=xiaomi_complete[['video', 'post_hour']]
y=xiaomi_complete['likes_count'].values

# Fit the model and print the result
X_train, X_test, y_train, y_test, clf, pred_labels, = mfunc(X, y, GaussianNB())

# Specify a size of the mesh to be used
mesh_size = 5
margin = 1

# Create a mesh grid on which we will run our model
x_min, x_max = X.iloc[:, 0].fillna(X.mean()).min() - margin, X.iloc[:, 0].fillna(X.mean()).max() + margin
y_min, y_max = X.iloc[:, 1].fillna(X.mean()).min() - margin, X.iloc[:, 1].fillna(X.mean()).max() + margin
xrange = np.arange(x_min, x_max, mesh_size)
yrange = np.arange(y_min, y_max, mesh_size)
xx, yy = np.meshgrid(xrange, yrange)

# Create classifier, run predictions on grid
Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
Z = Z.reshape(xx.shape)

# Specify traces
trace_specs = [
    #[X_train, y_train, 0, 'Train', 'brown'],
    #[X_train, y_train, 1, 'Train', 'aqua'],
    [X_test, y_test, 0, 'Test', 'red'],
    [X_test, y_test, 1, 'Test', 'blue']
]

# Build the graph using trace_specs from above
fig = go.Figure(data=[
    go.Scatter(
        x=X[y==label].iloc[:, 0], y=X[y==label].iloc[:, 1],
        name=f'{split} data, Actual Class: {label}',
        mode='markers', marker_color=marker
    )
    for X, y, label, split, marker in trace_specs
])

# Update marker size
fig.update_traces(marker_size=2, marker_line_width=0)

# Update axis range
fig.update_xaxes(range=[-100, 100])
fig.update_yaxes(range=[0,50])

# Update chart title and legend placement
fig.update_layout(title_text="Decision Boundary for Naive Bayes Model",
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1))

# Add contour graph
fig.add_trace(
    go.Contour(
        x=xrange,
        y=yrange,
        z=Z,
        showscale=True,
        colorscale='magma',
        opacity=1,
        name='Score',
        hoverinfo='skip'
    )
)

fig.show()

